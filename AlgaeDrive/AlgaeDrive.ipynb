{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Use continuous geographic, hydrological, climate, and meteorological datasets as input to train neural network(NN) and random forest(RF) models, to determine the major environmentl drivers of algae growth in lakes.\n",
        "Use Google Cloud Platform (GCP) to extract relative data from its [Earth Engine Dataset](https://developers.google.com/earth-engine/datasets) and run further NN models"
      ],
      "metadata": {
        "id": "dqS_PlyedNxv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Climate data preparation from Google Earth Engine"
      ],
      "metadata": {
        "id": "XNAX5x7a5DT2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##connect to google earth engine\n",
        "import ee\n",
        "service_account = '429406912917-compute@developer.gserviceaccount.com'\n",
        "credentials = ee.ServiceAccountCredentials(service_account, 'credentials.json')\n",
        "ee.Initialize(credentials)"
      ],
      "metadata": {
        "id": "enIjQIHAcw2S"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##prepare a mapping tool\n",
        "import folium\n",
        "def add_ee_layer(self, ee_image_object, vis_params, name):\n",
        "  map_id_dict = ee.Image(ee_image_object).getMapId(vis_params)\n",
        "  folium.raster_layers.TileLayer(\n",
        "      tiles=map_id_dict['tile_fetcher'].url_format,\n",
        "      attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "      name=name,\n",
        "      overlay=True,\n",
        "      control=True\n",
        "  ).add_to(self)\n",
        "\n",
        "folium.Map.add_ee_layer = add_ee_layer"
      ],
      "metadata": {
        "id": "84j_IV0TxVh8"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### extract climate data from ECMWF ERA 5\n",
        "##details of dataset: https://developers.google.com/earth-engine/datasets/catalog/ECMWF_ERA5_DAILY#description\n",
        "## https://cds.climate.copernicus.eu/cdsapp#!/software/app-era5-explorer?tab=overview\n",
        "sensor='ECMWF/ERA5/DAILY'\n",
        "start_date,end_date='2020-06-01','2020-07-01'\n",
        "aoi =ee.Geometry.Rectangle(-95.4, 48.88,-93.75, 49.77)\n",
        "era5 = ee.ImageCollection(sensor) \\\n",
        "       .filterBounds(aoi)  \\\n",
        "       .filterDate(ee.Date(start_date), ee.Date(end_date))\n",
        "era5_2mt = era5.select('mean_2m_air_temperature')\n",
        "print('Number of images', era5_2mt.size().getInfo())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IpZ6vJuKdtA0",
        "outputId": "451a757b-7acf-4f21-8b35-27436bb271cc"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of images 30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##preview the average imagery\n",
        "avgT=era5_2mt.mosaic().add(-273.15)  #kelvin to Celsius  c=k-273.15   #.clip(aoi)\n",
        "##result = ee.Image.expression('(slope * aspect) / 255', {'slope': slope,'aspect': aspect})\n",
        "print(\"Global temperature Jun 2020 monthly avg (C)\")\n",
        "\n",
        "## #show interactive map\n",
        "## vis_params = {\n",
        "##   'min': -30,\n",
        "##   'max': 40,\n",
        "##   'palette': ['blue', 'green', 'red']\n",
        "## }\n",
        "## center = aoi.centroid()\n",
        "## map = folium.Map(location=[center.coordinates().get(1).getInfo(),center.coordinates().get(0).getInfo()], zoom_start=5)\n",
        "## #map = folium.Map(location=[48.88,-95.4], zoom_start=10)\n",
        "## # Add the image layer to the map and display it.\n",
        "## map.add_ee_layer(avgT, vis_params, 'Avg Temp (c)')\n",
        "## map.add_child(folium.LayerControl())\n",
        "## display(map)\n",
        "\n",
        "##display a raster imagery\n",
        "from IPython.display import Image\n",
        "Image(url =avgT.updateMask(avgT.gt(-10)).getThumbURL({'min':0, 'max': 40, 'dimensions': 512,\n",
        "                'palette': ['blue', 'green', 'red']}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "uG6srK8id4oh",
        "outputId": "ab4f2660-6e52-4fef-902c-3ac99ced6ae9"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Global temperature Jun 2020 monthly avg (C)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"https://earthengine.googleapis.com/v1alpha/projects/earthengine-legacy/thumbnails/e8a403da6b212d5223eae804e6cb78cb-b667811af9a3c8e5d53524687a7e0588:getPixels\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## export to a test imagery to Google Cloud Platform (GCP) buckets \n",
        "img=era5_2mt.first()\n",
        "export_config = {\n",
        "    'image': img,\n",
        "    'region': aoi,\n",
        "    'crs': 'EPSG:4326',\n",
        "    'scale': 100000,    \n",
        "    'fileFormat': 'GeoTIFF',\n",
        "    'fileNamePrefix': 'ERA5_2mt',\n",
        "    'bucket':   'AlgaeDrive',  \n",
        "    'maxPixels':1e10,\n",
        "    'description': 'ERA5 climate data download: ER5 mean_2m_air_temperature'\n",
        "}\n",
        "task=ee.batch.Export.image.toCloudStorage(**export_config)\n",
        "#task=ee.batch.Export.image.toDrive(**export_config)\n",
        "task.start()\n",
        "import time\n",
        "time.sleep(10)\n",
        "print(task.status())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hax0WZmBeOgP",
        "outputId": "f32746fb-1d12-4c5a-e92f-afc03fcf4391"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'state': 'READY', 'description': 'ERA5 climate data download: ER5 mean_2m_air_temperature', 'creation_timestamp_ms': 1680028182464, 'update_timestamp_ms': 1680028182464, 'start_timestamp_ms': 0, 'task_type': 'EXPORT_IMAGE', 'id': 'UQFQZPCG5XE5SKJFP2QS4TKZ', 'name': 'projects/earthengine-legacy/operations/UQFQZPCG5XE5SKJFP2QS4TKZ'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##iterate all the image to export to GCP bucket\n",
        "def export_ERA5_to_GCPbucket(era5_dataset,aoi=ee.Geometry.Rectangle(-95.4, 48.88,-93.75, 49.77), bucket_name='AlgaeDrive',crs_epsg=4326,scale=1000,prdct_key=None):\n",
        "  img_list = era5_dataset.toList(era5_dataset.size())\n",
        "  nImages = img_list.size().getInfo()\n",
        "  for i in range(nImages):\n",
        "    img=ee.Image(img_list.get(i)).clip(aoi)\n",
        "    product_id=img.getInfo()['id'].replace('/','_')\n",
        "    export_config = {\n",
        "      'image': img.float(),\n",
        "      'region': aoi,\n",
        "      'crs': f'EPSG:{crs_epsg}',\n",
        "      'scale': scale,    \n",
        "      'fileFormat': 'GeoTIFF',\n",
        "      'fileNamePrefix': f'ERA5/{product_id}_{prdct_key}',\n",
        "      'bucket':   bucket_name,  \n",
        "      'maxPixels':1e10,\n",
        "      'description': f'ERA5 climate data download: ER5 product: {prdct_key}'\n",
        "    }\n",
        "    task=ee.batch.Export.image.toCloudStorage(**export_config)\n",
        "    task.start()"
      ],
      "metadata": {
        "id": "MSz8rVCRebWb"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##download all the related products\n",
        "dict_prdct={\n",
        "'mean_2m_air_temperature': 'Daily mean 2m air temperature',\n",
        "'total_precipitation': 'Daily total precipitation sums',\n",
        "'dewpoint_2m_temperature': \"Daily mean 2m dewpoint temperature\",\n",
        "'mean_sea_level_pressure':\"Daily mean sea-level pressure\",\n",
        "'surface_pressure': \"Daily mean surface pressure\",\n",
        "'u_component_of_wind_10m': \"Daily mean 10m u-component of wind\",\n",
        "}\n",
        "for prd in dict_prdct:\n",
        "    era5_dat = era5.select(prd)\n",
        "    print('Number of images', era5_dat.size().getInfo())\n",
        "    export_ERA5_to_GCPbucket(era5_dat,aoi=aoi, bucket_name='AlgaeDrive',crs_epsg=32615,scale=1000,prdct_key=prd)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzLLURWf5WPk",
        "outputId": "bfd6a265-9b0e-4210-ef21-682b5c465d13"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of images 30\n",
            "Number of images 30\n",
            "Number of images 30\n",
            "Number of images 30\n",
            "Number of images 30\n",
            "Number of images 30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. download satellite imagery of the same dates/AOI"
      ],
      "metadata": {
        "id": "qNBV8xLh5VMO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##download satellite imagery to bucket\n",
        "def export_OLCI_to_GCPbucket(dataset,aoi=None, bucket_name='AlgaeDrive',crs_epsg=4326,scale=300):\n",
        "  img_list = dataset.toList(dataset.size())\n",
        "  nImages = img_list.size().getInfo()\n",
        "  for i in range(nImages):\n",
        "    img=ee.Image(img_list.get(i)).clip(aoi)\n",
        "    product_id=img.get('PRODUCT_ID').getInfo()\n",
        "    export_config = {\n",
        "      'image': img.float(),\n",
        "      'region': aoi,\n",
        "      'crs': f'EPSG:{crs_epsg}',\n",
        "      'scale': scale,    #default in 300 pixels of OLCI\n",
        "      'fileFormat': 'GeoTIFF',\n",
        "      'fileNamePrefix': f'OLCI/{product_id}',\n",
        "      'bucket':   bucket_name,  \n",
        "      'maxPixels':1e10,\n",
        "      'description': 'ESA OLCI ocean color satellite imagery'\n",
        "    }\n",
        "    task=ee.batch.Export.image.toCloudStorage(**export_config)\n",
        "    task.start()\n",
        "\n",
        "crs_epsg=32615  #the epsg code for projection\n",
        "start_date,end_date='2020-06-01','2020-07-01'\n",
        "aoi =ee.Geometry.Rectangle(-95.4, 48.88,-93.75, 49.77)\n",
        "sensor='COPERNICUS/S3/OLCI'\n",
        "imgs=ee.ImageCollection(sensor) \\\n",
        "       .filterBounds(aoi)  \\\n",
        "       .filterDate(ee.Date('2022-09-01'), ee.Date('2022-09-03'))\n",
        "export_OLCI_to_GCPbucket(imgs,aoi=aoi, bucket_name='AlgaeDrive',crs_epsg=4326,scale=300)"
      ],
      "metadata": {
        "id": "wKI3jYyS84pn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## download other similar satellite products\n",
        "\n",
        "###...\n",
        "\n",
        "## download geographic dataset e.g.,  DEM\n",
        " \n",
        "### ...\n",
        "\n",
        "## download the output variable (chlorophyll-concentration) from EOLakewatch\n",
        "## https://hpfx.collab.science.gc.ca/~sntl123/GeoTIFFs/LoW/  (only need 2016-present geotiff files)"
      ],
      "metadata": {
        "id": "JsAL_ayX-Lu-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. build machine learning models "
      ],
      "metadata": {
        "id": "6kRAeA3s-Yka"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1  Preparation & data cleaning"
      ],
      "metadata": {
        "id": "OZSY4zgb-6n3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##  some auxilary functions \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "def sci_notation(number, sig_fig=2):\n",
        "    \"\"\"\n",
        "    convert scientific numbers to \"###x10^###\" type\n",
        "    \"\"\"\n",
        "    ret_string = \"{0:.{1:d}e}\".format(number, sig_fig)\n",
        "    a,b = ret_string.split(\"e\")\n",
        "    b = int(b) #removed leading \"+\" and strips leading zeros too.\n",
        "    return a + 'x10$^{' + str(b)+'}$'\n",
        "\n",
        "##metrics\n",
        "EPSILON = 1e-10\n",
        "def _error(actual: np.ndarray, predicted: np.ndarray):\n",
        "    \"\"\" Simple error \"\"\"\n",
        "    return predicted - actual\n",
        "def _percentage_error(actual: np.ndarray, predicted: np.ndarray, bZero=False):\n",
        "    \"\"\"\n",
        "    Percentage error\n",
        "    Note: result is NOT multiplied by 100\n",
        "    \"\"\"\n",
        "    if not bZero:  \n",
        "        return _error(actual, predicted) / (actual + EPSILON)\n",
        "    else: ##to avoid the extreme high percentage at values close to zero       \n",
        "        Tmin=np.max([np.nanmin(actual)+1e-4*np.ptp(actual), np.nanpercentile(actual,2)])  # <2% or 0.1%+min, which ever is larger\n",
        "        val=_error(actual, predicted) / (actual + EPSILON)\n",
        "        val[actual<Tmin]=np.nan  #set the values lower than min threshold as invalid\n",
        "        return val\n",
        "def stats_mse(actual: np.ndarray, predicted: np.ndarray):\n",
        "    \"\"\" Mean Squared Error \"\"\"\n",
        "    return np.mean(np.square(_error(actual, predicted)))\n",
        "\n",
        "def stats_rmse(actual: np.ndarray, predicted: np.ndarray):\n",
        "    \"\"\" Root Mean Squared Error \"\"\"\n",
        "    return np.sqrt(stats_mse(actual, predicted))\n",
        "def stats_bias(actual: np.ndarray, predicted: np.ndarray):\n",
        "    return np.sum(_error(actual, predicted)) / len(actual)\n",
        "def stats_rsquare(actual: np.ndarray, predicted: np.ndarray):\n",
        "    r=np.corrcoef(predicted, actual)[0,1]\n",
        "    return r**2\n",
        "def stats_mdape(actual: np.ndarray, predicted: np.ndarray):\n",
        "    \"\"\"\n",
        "    Median Absolute Percentage Error\n",
        "    Note: result is NOT multiplied by 100\n",
        "    \"\"\"\n",
        "    return np.median(np.abs(_percentage_error(actual, predicted)))"
      ],
      "metadata": {
        "id": "-_WfsrVH-YJN"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def WaterIndex(Rrs,Bands=[681, 709, 753]):\n",
        "    \"\"\"\n",
        "    cacluate line height water index\n",
        "    \"\"\"\n",
        "    if np.any(np.isnan(Rrs)):  #data validation\n",
        "        return np.nan\n",
        "    else:\n",
        "        idx=Rrs[1]-Rrs[0]- (Bands[1]-Bands[0])/(Bands[2]-Bands[0])*(Rrs[2]-Rrs[0])\n",
        "    return idx\n",
        "def plot_performance(y_test,y_pred,x_range=[2,7.5], model_name='', model_str=''):\n",
        "    import matplotlib.pyplot as plt\n",
        "    from sklearn.metrics import mean_squared_error\n",
        "    #mse = mean_squared_error(y_test, y_pred)\n",
        "    #rmse = mse**(0.5)\n",
        "    #print(\"MSE: %.2f\" % mse)\n",
        "    #print(\"RMSE: %.2f\" % rmse)\n",
        "    x = np.linspace(x_range[0],x_range[1],100)\n",
        "    plt.figure()\n",
        "    stats={'rmse':stats_rmse(y_test,y_pred),'bias':stats_bias(y_test,y_pred),'mdape':stats_mdape(y_test,y_pred),'rsquare':stats_rsquare(y_test,y_pred)}\n",
        "    print(stats)\n",
        "    plt.title(model_name,fontweight='bold',fontsize=12,color= 'k')\n",
        "    plt.plot(y_test,y_pred,'b.')\n",
        "    plt.plot(x,x,'r')\n",
        "    plt.text(0.7,0.05, \" R$^2$={:.3f} \\n RMSE={:.3f} \\n BIAS={:.3f} \\n MAPE={:.3f}\".format\n",
        "             (stats['rsquare'],stats['rmse'], stats['bias'],stats['mdape']),\n",
        "             fontsize=12,weight='bold',transform=plt.gca().transAxes)\n",
        "    plt.text(0.05, 0.9,f'NN: {model_str}',fontsize=12,weight='bold',transform=plt.gca().transAxes)\n",
        "    #plt.xlabel('observed Chl [\\u03BCg/L]'),plt.ylabel('predicted Chl [\\u03BCg/L]')\n",
        "    plt.xlabel('observed density logged [cells/mL]'),plt.ylabel('predicted density logged [cells/mL]')\n",
        "    plt.grid()\n",
        "    plt.plot(x, x,'r')\n",
        "\n",
        "def lightGBM(X_train, y_train,X_test, y_test,bPlot=True,bRegression=False , N=5,sample_weights=None):\n",
        "    ##implementation 3: LightGBM\n",
        "    import lightgbm as lgb\n",
        "    import random\n",
        "    #import joblib\n",
        "    #import numpy as np\n",
        "    #from pathlib import Path\n",
        "    #import typer\n",
        "    # defining parameters \n",
        "    params = {\n",
        "        'task': 'train', \n",
        "        'boosting': 'gbdt',\n",
        "        'objective': 'regression' if bRegression else 'multiclass' ,\n",
        "        'num_class': 1 if bRegression else 5,\n",
        "        'num_leaves': 10,  #31,  #depth\n",
        "        'learning_rate': 0.05,\n",
        "        'feature_fraction': 0.9,\n",
        "        'bagging_fraction': 0.8,\n",
        "        #'bagging_freq': 5,\n",
        "        'metric': 'rmse' if bRegression else 'multi_logloss' ,  #{'l2','l1'},\n",
        "        'verbose': -1\n",
        "    }\n",
        "    # loading data\n",
        "    if sample_weights is None:lgb_train = lgb.Dataset(X_train, y_train)\n",
        "    else: lgb_train = lgb.Dataset(X_train, y_train, weight=sample_weights)\n",
        "    lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)\n",
        "    \n",
        "    model_list=list()\n",
        "    for idx in range(N):   \n",
        "        # fitting the model\n",
        "        params['seed']=random.randint(1, 1000)  #use a random number in 1-1000 as seed\n",
        "        model = lgb.train(params,train_set=lgb_train,valid_sets=lgb_eval,early_stopping_rounds=30,verbose_eval=False)\n",
        "        model_list.append(model)\n",
        "    \n",
        "    # Make predictions for the test data using each model\n",
        "    predictions = []\n",
        "    for model in model_list:\n",
        "        prediction = model.predict(X_test)\n",
        "        predictions.append(prediction)\n",
        "    \n",
        "    # Average the predicted probabilities across all models\n",
        "    y_pred_gbm = np.mean(predictions, axis=0)\n",
        "    ### prediction\n",
        "    ##y_pred_gbm = model.predict(X_test)       \n",
        "        \n",
        "    from sklearn.metrics import mean_squared_error\n",
        "    from sklearn.metrics import confusion_matrix\n",
        "    if  not bRegression:\n",
        "        y_pred_gbm = np.argmax(y_pred_gbm, axis=1)\n",
        "        rmse = mean_squared_error(y_test, y_pred_gbm)**(0.5)\n",
        "        print('=== classification Accuracy:', np.sum(y_pred_gbm == y_test) / len(y_test),'; RMSE:',rmse)\n",
        "        #print('classification Accuracy:', np.sum(y_pred_gbm == y_test) / len(y_test))\n",
        "        cm = confusion_matrix(y_test, y_pred_gbm)\n",
        "        print(\"confusion matrix: \\n\",cm)\n",
        "    else:\n",
        "        # accuracy check\n",
        "        if bPlot: plot_performance(y_test,y_pred_gbm,model_name='lightGBM',model_str=params['num_leaves'])\n",
        "        \n",
        "        y_test = np.digitize(y_test, bins=np.log10([20000 , 100000, 1000000, 10000000]))\n",
        "        y_pred_gbm = np.digitize(y_pred_gbm, bins=np.log10([20000 , 100000, 1000000, 10000000]))\n",
        "        rmse = mean_squared_error(y_test, y_pred_gbm)**(0.5)\n",
        "        print('=== classification Accuracy:', np.sum(y_pred_gbm == y_test) / len(y_test),'; RMSE:',rmse)\n",
        "        cm = confusion_matrix(y_test, y_pred_gbm)\n",
        "        print(\"confusion matrix: \\n\",cm)\n",
        "    \n",
        "    # Get the feature importances\n",
        "    importance = model.feature_importance()\n",
        "    # Get the feature names\n",
        "    feature_names = X_train.columns\n",
        "    # Create a dictionary of feature names and importance scores\n",
        "    feature_importance_dict = dict(zip(feature_names, importance))\n",
        "    ## Print the feature importances in descending order\n",
        "    print(sorted(feature_importance_dict.items(), key=lambda x: x[1], reverse=True))\n",
        "\n",
        "    return model\n",
        "\n",
        "def RandomForeset(X_train, y_train,X_test,y_test,bPlot=True,bRegression=False ,N=1,sample_weights=None):\n",
        "    #Import Random Forest Model\n",
        "    from sklearn.ensemble import RandomForestRegressor,RandomForestClassifier\n",
        "    \n",
        "    ###normalize the samples weight to make sum as 1\n",
        "    #if not (sample_weights is None):\n",
        "    #  from sklearn.preprocessing import normalize\n",
        "    #  sample_weights = np.sqqueze(  normalize([np.array(sample_weights)], norm='l1'))\n",
        "\n",
        "    model_list=list()\n",
        "    for idx in range(N):\n",
        "        #from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "        if bRegression:  \n",
        "          rfr = RandomForestRegressor(n_estimators=100)\n",
        "        else:  \n",
        "          rfr = RandomForestClassifier(n_estimators=100) #, random_state=42; if set this random_state , then model give same output?\n",
        "        #print(rfr)\n",
        "        \n",
        "        '''\n",
        "        RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
        "                              max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
        "                              max_samples=None, min_impurity_decrease=0.0,\n",
        "                              min_samples_leaf=1,\n",
        "                              min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
        "                              n_estimators=100, n_jobs=None, oob_score=False,\n",
        "                              random_state=None, verbose=0, warm_start=False) \n",
        "        '''\n",
        "        if sample_weights is None:rfr.fit(X_train, y_train)\n",
        "        else: rfr.fit(X_train, y_train, sample_weight=sample_weights)\n",
        "        #score = rfr.score(X_train, y_train)\n",
        "        #print(\"training dataset R-squared:\", score) \n",
        "        model_list.append(rfr)\n",
        "\n",
        "    # Make predictions for the test data using each model\n",
        "    predictions = []\n",
        "    for model in model_list:\n",
        "        if bRegression: prediction = model.predict(X_test)\n",
        "        else: prediction = model.predict_proba(X_test)\n",
        "        predictions.append(prediction)\n",
        "    \n",
        "    # Average the predicted probabilities across all models\n",
        "    ypred_rf = np.mean(predictions, axis=0)\n",
        "    \n",
        "    # Get the predicted class for each sample\n",
        "    if not bRegression: ypred_rf = np.argmax(ypred_rf, axis=1)\n",
        "\n",
        "    from sklearn.metrics import mean_squared_error\n",
        "    from sklearn.metrics import confusion_matrix\n",
        "    if bRegression: \n",
        "        if bPlot: plot_performance(y_test,ypred_rf,model_name='RandomForeset',model_str='')\n",
        "\n",
        "        y_test = np.digitize(y_test, bins=np.log10([20000 , 100000, 1000000, 10000000]))\n",
        "        ypred_rf = np.digitize(ypred_rf, bins=np.log10([20000 , 100000, 1000000, 10000000]))\n",
        "        rmse = mean_squared_error(y_test, ypred_rf)**(0.5)\n",
        "        print('=== classification Accuracy:', np.sum(ypred_rf == y_test) / len(y_test),'; RMSE:',rmse)\n",
        "        cm = confusion_matrix(y_test, ypred_rf)\n",
        "        print(\"confusion matrix: \\n\",cm)\n",
        "        \n",
        "    else:\n",
        "        rmse = mean_squared_error(y_test, ypred_rf)**(0.5)\n",
        "        #region_rmse=lambda y_test, y_pred: \n",
        "        print('=== classification Accuracy:', np.sum(ypred_rf == y_test) / len(y_test),'; RMSE:',rmse)\n",
        "        cm = confusion_matrix(y_test, ypred_rf)\n",
        "        print(\"confusion matrix: \\n\",cm)\n",
        "        \n",
        "    return rfr    \n",
        "\n",
        "def TensorFlow(X_train, y_train,X_test,y_test,bPlot=True,Ntraining=1,nodes=[64,64],drop_rate=0.3,reg_factor=1e-8,batch_size=128,epochs=64):\n",
        "    ## Impelentation : training with ANN via tensorflow\n",
        "    ## TRAINING WITH tensorflow, adapted from : https://github.com/chqzeng/MODISNN/blob/main/MODISNN_TF/MODISNN_TF_training.py\n",
        "    import tensorflow as tf\n",
        "    ## ---- NN training  : [X_train, X_test, y_train, y_test] ---\n",
        "    # Create a `Sequential` model and add Dense layers \n",
        "    model_list=list()\n",
        "      #hidden layer nodes\n",
        "    #repeated training times\n",
        "    regularizer=tf.keras.regularizers.L1(reg_factor)  \n",
        "    init=tf.keras.initializers.RandomUniform(minval=-0.05, maxval=0.05, seed=None)\n",
        "    model = tf.keras.models.Sequential()\n",
        "    model.add(tf.keras.Input(shape=(X_train.shape[1],), name=\"input\"))\n",
        "    for idx in range(len(nodes)):\n",
        "        #model.add(tf.keras.layers.Dense(nodes[idx], activation='relu', name=\"dense{}\".format(idx)))\n",
        "        model.add(tf.keras.layers.Dense(nodes[idx],kernel_regularizer=regularizer,kernel_initializer = init, activation='relu', name=\"dense{}\".format(idx))) \n",
        "        ##activation='sigmoid'  'relu' 'tanh'  activation=tf.keras.layers.LeakyReLU(alpha=0.01)  tf.keras.layers.PReLU() 'elu'; in general SELU > ELU > leaky ReLU (and its variants) > ReLU > tanh > logistic\n",
        "        model.add(tf.keras.layers.Dropout(drop_rate))\n",
        "    Nout=1 if len(y_train.shape)==1 else y_train.shape[1]  #if output 1-dimension then, output dimension=1; if 2-d output, then output dimension=shape[1]\n",
        "    model.add(tf.keras.layers.Dense(Nout,  name=\"output\",activation='relu'))\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False)\n",
        "    model.compile(loss='mse', optimizer=opt,metrics=['accuracy'])  # metrics=['accuracy'])  #optimizer='adam'\n",
        "    Wreset = model.get_weights()     ##store the param for reset\n",
        "    #print('=== repeat training (100%): ', end=\"\")\n",
        "    for idx in range(0,Ntraining):\n",
        "        history = model.fit(X_train,y_train,batch_size=batch_size,epochs=epochs,verbose=False) #,validation_split = 0.3)\n",
        "        model_list.append({\"idx\":idx,\"weights\":model.get_weights(),\"history\":history} ) #store the model history\n",
        "        #model.save('__test__.model')  #save the full keras model;  load later as: model=keras.models.load_model(\"__test__.model\")\n",
        "        print(model.get_weights()[0])\n",
        "        ##show stats\n",
        "        Y_pred=np.squeeze(model.predict(X_test,verbose=False))\n",
        "        Y_pred[Y_pred>8]=8\n",
        "        Y_pred[Y_pred<0]=0\n",
        "        ## calculate stats\n",
        "        stats={'rmse':stats_rmse(y_test,Y_pred),'bias':stats_bias(y_test,Y_pred),'mdape':stats_mdape(y_test,Y_pred),'rsquare':stats_rsquare(y_test,Y_pred)}  #metrics=('rmse', 'bias', 'mdape', 'rsquare'))\n",
        "        print(stats)\n",
        "        model.set_weights(Wreset)\n",
        "    \n",
        "    ##  ANN prediction\n",
        "    Y_pred_list=np.ndarray([Ntraining,y_test.shape[0]], dtype=float)\n",
        "    for idx in range(0,Ntraining):\n",
        "        model.set_weights(model_list[idx]['weights'])\n",
        "        Y_pred_list[idx,:]=np.squeeze(model.predict(X_test,verbose=False))\n",
        "    ## using median prediction of many training attemps (similar to random forest) \n",
        "    Y_pred=np.median(Y_pred_list,axis=0)  \n",
        "    ##force range\n",
        "    if bPlot: plot_performance(y_test,Y_pred,model_name='Tensorflow',model_str=str(nodes))\n",
        "##--------------------"
      ],
      "metadata": {
        "id": "stPuZNdZBAzz"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "##load existing data; initial datasets\n",
        "work_dir='./'\n",
        "sample_data = pd.read_csv(os.path.join(work_dir,\"training.csv\"))  #pd.read_csv('../Sentinel3_TOA_training.csv')\n",
        "\n",
        "## load climate data\n",
        "#data_HRRR = pd.read_csv(os.path.join(work_dir, 'DEM_and_HRRR/HRRR.csv'))\n",
        "#data_DEM = pd.read_csv(os.path.join(work_dir, 'DEM_and_HRRR/DEM.csv'))\n",
        "#train_labels = pd.read_csv(os.path.join(work_dir, 'Code/train_labels.csv'))\n",
        "#metadata = pd.read_csv(os.path.join(work_dir, 'Code/metadata.csv'))\n",
        "\n",
        "# Import train_test_split function\n",
        "from sklearn.model_selection import train_test_split\n",
        "inputFeatures=['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'NDVI', 'NDCI', 'B8AB4', 'B3B2','dem','test2',\n",
        "               'mean_2m_air_temperature',\n",
        "'total_precipitation    ',\n",
        "'dewpoint_2m_temperature',\n",
        "'mean_sea_level_pressure',\n",
        "'surface_pressure       ',\n",
        "'u_component_of_wind_10m',]\n",
        "X = sample_data[inputFeatures]  # Features\n",
        "#X = results[['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7','NDVI', 'NDCI', 'B8AB4', 'B3B2','test1','test2']]  # Features\n",
        "#results['chl_a'][results.chl_a==0] = 100    #why?? to avoid -inf?\n",
        "y=sample_data['chl_a']    # the output variable is the chlorophyll a concentration from our EOLakeWatch\n",
        "\n",
        "# Split dataset into training set and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) # 70% training and 30% test"
      ],
      "metadata": {
        "id": "tZhlHqFi_SVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zAEX-9gJBpTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Model training"
      ],
      "metadata": {
        "id": "qvW7rDxnAg32"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## ----------------the actual training  ----------------\n",
        "\n",
        "## Split dataset into training set and test set;  reokaced by the block below, since the train_test_split does not return flag to further split weight column\n",
        "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) # 70% training and 30% test\n",
        "sensor_model='S2'\n",
        "bCategory=True\n",
        "\n",
        "T=700 ## split the random numbers in range [0,1000)  as training and test\n",
        "flt_training = np.random.randint(1000, size=sample_data.shape[0]) < T\n",
        "#sum(flt_training)/(1.0*sample_data.shape[0]), sum(~flt_training)/(1.0*sample_data.shape[0]), sample_data.shape\n",
        "sample_weights=sample_data.loc[flt_training,'sample_weight']\n",
        "X_train, X_test, y_train, y_test = X.loc[flt_training,:],X.loc[~flt_training,:], y[flt_training],y[~flt_training]\n",
        "\n",
        "print(f'--------------------light GBM : {sensor_model} : {\"Classfication\" if bCategory else \"Regression\"}------------------')\n",
        "gbm=lightGBM(X_train, y_train,X_test, y_test,bPlot=True,sample_weights=sample_weights,bRegression=not bCategory,N=5)  #N=5 means train the model 5 times & average the prediction\n",
        "\n",
        "print(f'--------------------Random Forest : {sensor_model} : {\"Classfication\" if bCategory else \"Regression\"}------------------')\n",
        "rfr=RandomForeset(X_train, y_train,X_test, y_test,bPlot=True,sample_weights=sample_weights,bRegression=not bCategory,N=5)\n",
        "\n",
        "print(f'--------------------tensorflow  : {sensor_model} : {\"Classfication\" if bCategory else \"Regression\"}------------------')\n",
        "TensorFlow(X_train, y_train,X_test,y_test,Ntraining=3,bPlot=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_KzdjmL7_-ze",
        "outputId": "55474966-1374-4961-b927-d159521d37b0"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean_2m_air_temperature\n",
            "total_precipitation\n",
            "dewpoint_2m_temperature\n",
            "mean_sea_level_pressure\n",
            "surface_pressure\n",
            "u_component_of_wind_10m\n"
          ]
        }
      ]
    }
  ]
}